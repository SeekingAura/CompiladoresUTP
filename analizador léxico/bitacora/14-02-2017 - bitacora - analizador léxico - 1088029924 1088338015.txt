Estudiantes: Carlos Arturo Moreno Tabares - 1088029924
Karen Stefanny Lopez Segura - 1088338015

Se verifica la grámatica antes elaborada y vuelve a retomar el codigo "test léxico.py" (el cual se inició el dia 09-02-2017) para realizar pruebas del Lexer y configurarla para que entienda el lenguaje de Lola; se tuvo que realizar consultas sobre las expresiones regulares y practicar con las funciones de lexer para que pueda entender y comprender los tokens / palabras de un texto dado.

En la clase (la cual no hubo, por problemas con el salón) se consultó con el Ingeniero Angel Augusto sobre ¿Qué es lo que deberia hacer en si el léxico, como es la entrega? y con ello se aclaró lo que debe llevar la entrega para 18-02-2017, el programa debe de:

- Tomar cada palabra e identiiarla indicando lo tomado a que token pertenece
- Indicar la linea y columna de token que identifica
- estar elaborado con sus respectivas expresiones regulares, al menos las basicas para un programa simple en lola

Del codigo ejemplo que se tomó y probó de https://github.com/dabeaz/sly del día 09-02-2017 (guardado como test léxico.py) se realizarón avances y se declararón varios tokens, las expresiones regulares de alguna palabras reservadas y los comentarios; tambien de que la toma del string a aplicar lexer.tokenize se elaboró que tomará de un archivo .txt y lo convirties para luego mandar linea por linea asi:

parte de archivo "test léxico" 

#--#

data= open('test.txt')
	lexer = CalcLexer()
	i=1
	j=1
	for linea in data:
		
		j=1
		for tok in lexer.tokenize(linea):
			print("Linea {}, coluna{}".format(i, j))
			print('tipo={}, valor={}'.format(tok.type, tok.value))
			
			j+=1
		i+=1

#--#

lo que hacia es que de todo el test.txt mandaba linea por linea el lexer.tokenize y claculaba apartir de las iteraciones de los for la linea y columna, cosa que la columna no quedaba bien, la fila si coincidia. El codigo quedó asi:

archivo "test léxico.py":
#--#

# -----------------------------------------------------------------------------
# calc.py
# -----------------------------------------------------------------------------

from sly import Lexer, Parser

class CalcLexer(Lexer):
	#keywords = { 
	#	'BEGIN', 'CONST', 'END', 'IN', 'INOUT', 'MODULE', 'OUT', 'REG', 'TS', 'OC', 'TYPE', 'VAR', 'DIV', 'MOD', 'MUX', 'LATCH', 'SR', 'IF', 'THEN', 'ELSIF', 'FOR', 'DO', 
	#}
	tokens = {
		'identificador', 'NUMBER', 'MODULE', 'TYPE', 'IN', 'OUT', 'BEGIN', 'END', 'FOR', 'IN', 'OUT', 'FACTOR',
	}
	ignore = ' \t'
	
	literals = { '~', '&', '|', '^', '+', '-', '*', '=', '#', '<', '<=', '>', '>=', '(', ')', '[', ']', '{', '}', '->', '.', ',', ';', ':', ':=', "'", '!', '?', '..'}
	
	# Tokens - Reservados
	
	OUT = 'OUT'
	IN = r'IN'
	TYPE = r'TYPE'
	MODULE = r'MODULE'
	
	# Tokens - otros
	identificador = r'[a-zA-Z_][a-zA-Z0-9_]*' 
	
	@_(r'\d+')#expresión regular que trabaja [0-9]
	def NUMBER(self, t):
		t.value = int(t.value)
		return t
	
	
	@_(r'\n+')
	def newline(self, t):
		self.lineno += t.value.count('\n')
		return

	def error(self, value):
		print("Illegal character '%s'" % value[0])
		self.index += 1
		
	@_(r'[(][*].*[*][)]')
	def COMMENT(self, t):
		return
	
if __name__ == '__main__':
	data= open('test.txt')
	lexer = CalcLexer()
	i=1
	j=1
	for linea in data:
		
		j=1
		for tok in lexer.tokenize(linea):
			print("Linea {}, coluna{}".format(i, j))
			print('tipo={}, valor={}'.format(tok.type, tok.value))
			
			j+=1
		i+=1
	
#--#